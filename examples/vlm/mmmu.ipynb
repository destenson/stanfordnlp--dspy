{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "from dspy.datasets import DataLoader\n",
    "from dspy.evaluate.metrics import answer_exact_match\n",
    "from typing import List\n",
    "from dspy.evaluate import Evaluate\n",
    "\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "def debug_exact_match(example, pred, trace=None, frac=1.0):\n",
    "    print(example.inputs())\n",
    "    print(example.answer)\n",
    "    print(pred)\n",
    "    # print(trace)\n",
    "    # print(frac)\n",
    "    return answer_exact_match(example, pred, trace, frac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lm = dspy.LM(model=\"openai/Qwen/Qwen2-VL-7B-Instruct\", api_base=\"http://localhost:8000/v1\", api_key=\"sk-fake-key\", max_tokens=5000)\n",
    "lm = dspy.LM(model=\"openai/gpt-4o-mini\")\n",
    "\n",
    "dspy.settings.configure(lm=lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "input_keys = tuple([f\"image_{i}\" for i in range(1, 3)] + [\"question\", \"options\"])\n",
    "subsets = ['Accounting', 'Agriculture', 'Architecture_and_Engineering', 'Art', 'Art_Theory', 'Basic_Medical_Science', 'Biology', 'Chemistry', 'Clinical_Medicine', 'Computer_Science', 'Design', 'Diagnostics_and_Laboratory_Medicine', 'Economics', 'Electronics', 'Energy_and_Power', 'Finance', 'Geography', 'History', 'Literature', 'Manage', 'Marketing', 'Materials', 'Math', 'Mechanical_Engineering', 'Music', 'Pharmacy', 'Physics', 'Psychology', 'Public_Health', 'Sociology']\n",
    "\n",
    "devset = []\n",
    "valset = []\n",
    "with ThreadPoolExecutor(max_workers=len(subsets)) as executor:\n",
    "    def load_dataset(subset_index_subset):\n",
    "        subset_index, subset = subset_index_subset\n",
    "        dataset = DataLoader().from_huggingface(\"MMMU/MMMU\", subset, split=[\"dev\", \"validation\"], input_keys=input_keys)\n",
    "        return subset_index, dataset[\"dev\"], dataset[\"validation\"]\n",
    "    \n",
    "    results = list(executor.map(load_dataset, enumerate(subsets)))\n",
    "    \n",
    "    results.sort(key=lambda x: x[0])\n",
    "    \n",
    "    for _, dev, val in results:\n",
    "        devset.extend(dev)\n",
    "        valset.extend(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image counts in devset:\n",
      "0 image(s): 0 examples\n",
      "1 image(s): 0 examples\n",
      "2 image(s): 4 examples\n",
      "3 image(s): 0 examples\n",
      "4 image(s): 0 examples\n",
      "5 image(s): 0 examples\n",
      "\n",
      "Image counts in valset:\n",
      "0 image(s): 0 examples\n",
      "1 image(s): 0 examples\n",
      "2 image(s): 43 examples\n",
      "3 image(s): 0 examples\n",
      "4 image(s): 0 examples\n",
      "5 image(s): 0 examples\n",
      "\n",
      "Multiple choice questions in devset:\n",
      "4 out of 4\n",
      "\n",
      "Multiple choice questions in valset:\n",
      "42 out of 43\n",
      "Example({'id': 'dev_Art_Theory_3', 'question': 'Church interiors from this time period typically were covered with <image 1> <image 2>', 'options': \"['timber roofs', 'quadripartite vaults', 'pendentive domes', 'masonry barrel vaults']\", 'explanation': '', 'image_1': <PIL.PngImagePlugin.PngImageFile image mode=RGBA size=933x609 at 0x7E259C230790>, 'image_2': <PIL.PngImagePlugin.PngImageFile image mode=RGBA size=933x737 at 0x7E259FF6C490>, 'image_3': None, 'image_4': None, 'image_5': None, 'image_6': None, 'image_7': None, 'img_type': \"['Sculpture']\", 'answer': 'A', 'topic_difficulty': 'Easy', 'question_type': 'multiple-choice', 'subfield': 'Visual Culture'}) (input_keys={'options', 'image_1', 'image_2', 'question'})\n",
      "Example({'id': 'dev_Art_Theory_3', 'question': 'Church interiors from this time period typically were covered with <image 1> <image 2>', 'options': \"['timber roofs', 'quadripartite vaults', 'pendentive domes', 'masonry barrel vaults']\", 'explanation': '', 'image_1': <PIL.PngImagePlugin.PngImageFile image mode=RGBA size=933x609 at 0x7E259C230790>, 'image_2': <PIL.PngImagePlugin.PngImageFile image mode=RGBA size=933x737 at 0x7E259FF6C490>, 'image_3': None, 'image_4': None, 'image_5': None, 'image_6': None, 'image_7': None, 'img_type': \"['Sculpture']\", 'answer': 'A', 'topic_difficulty': 'Easy', 'question_type': 'multiple-choice', 'subfield': 'Visual Culture', 'answer_choices': \"['A. timber roofs', 'B. quadripartite vaults', 'C. pendentive domes', 'D. masonry barrel vaults']\"}) (input_keys={'image_1', 'answer_choices', 'image_2', 'options', 'question'})\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "def count_images(dataset):\n",
    "    image_counts = {i: 0 for i in range(6)}  # Initialize counts for 0 to 2 images\n",
    "    for example in dataset:\n",
    "        count = sum(1 for key in example.inputs().keys() if key.startswith('image_') and example.inputs()[key] is not None)\n",
    "        image_counts[count] += 1\n",
    "    return image_counts\n",
    "\n",
    "def count_multiple_choice_questions(dataset):\n",
    "    return sum(1 for example in dataset if example[\"question_type\"] == \"multiple-choice\")\n",
    "max_images = 5\n",
    "\n",
    "num_images = 2\n",
    "\n",
    "devset_filtered = [example for example in devset if sum(1 for key in example.inputs().keys() if key.startswith('image_') and example.inputs()[key] is not None) == num_images]\n",
    "valset_filtered = [example for example in valset if sum(1 for key in example.inputs().keys() if key.startswith('image_') and example.inputs()[key] is not None) == num_images]\n",
    "\n",
    "devset_image_counts = count_images(devset_filtered)\n",
    "valset_image_counts = count_images(valset_filtered)\n",
    "\n",
    "devset_multiple_choice_questions = count_multiple_choice_questions(devset_filtered)\n",
    "valset_multiple_choice_questions = count_multiple_choice_questions(valset_filtered)\n",
    "\n",
    "print(\"Image counts in devset:\")\n",
    "for count, num_examples in devset_image_counts.items():\n",
    "    print(f\"{count} image(s): {num_examples} examples\")\n",
    "\n",
    "print(\"\\nImage counts in valset:\")\n",
    "for count, num_examples in valset_image_counts.items():\n",
    "    print(f\"{count} image(s): {num_examples} examples\")\n",
    "\n",
    "print(\"\\nMultiple choice questions in devset:\")\n",
    "print(devset_multiple_choice_questions, \"out of\", len(devset_filtered))\n",
    "print(\"\\nMultiple choice questions in valset:\")\n",
    "print(valset_multiple_choice_questions, \"out of\", len(valset_filtered))\n",
    "\n",
    "def convert_multiple_choice_to_letter(dataset):\n",
    "    new_dataset = []\n",
    "    for example in dataset:\n",
    "        if example[\"question_type\"] == \"multiple-choice\":\n",
    "            # print(example[\"options\"])\n",
    "            options = ast.literal_eval(example[\"options\"])\n",
    "            example[\"answer_choices\"] = str([chr(65 + i) + \". \" + option for i, option in enumerate(options)])\n",
    "        else:\n",
    "            example[\"answer_choices\"] = str(ast.literal_eval(example[\"options\"]))\n",
    "\n",
    "        updated_example = example.with_inputs(*example.inputs().keys(), \"answer_choices\")\n",
    "        new_dataset.append(updated_example)\n",
    "    return new_dataset\n",
    "\n",
    "print(devset_filtered[0])\n",
    "updated_devset = convert_multiple_choice_to_letter(devset_filtered)\n",
    "print(updated_devset[0])\n",
    "updated_valset = convert_multiple_choice_to_letter(valset_filtered)\n",
    "\n",
    "# print(devset[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(devset))\n",
    "# print(devset)\n",
    "\n",
    "class MMMUSignature(dspy.Signature):\n",
    "    \"\"\"Output a rationale and the answer to a multiple choice question about an image with the letter of the correct answer, if present, otherwise the exact answer.\"\"\"\n",
    "\n",
    "    question: str = dspy.InputField(desc=\"A question about the image(s)\")\n",
    "    image_1: dspy.Image = dspy.InputField(desc=\"An image relating to the shown problem\")\n",
    "    image_2: dspy.Image = dspy.InputField(desc=\"An image relating to the shown problem\")\n",
    "    answer_choices: List[str] = dspy.InputField(desc=\"The answer options for the question\")\n",
    "    answer: str = dspy.OutputField(desc=\"The single letter of the correct answer. Do not include the entire answer or a period at the end.\")\n",
    "\n",
    "class MMMUModule(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.predictor = dspy.ChainOfThought(MMMUSignature)\n",
    "\n",
    "    def __call__(self, **kwargs):\n",
    "        return self.predictor(**kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs_optimizer_no_labeled = dspy.BootstrapFewShotWithRandomSearch(\n",
    "    metric=answer_exact_match,\n",
    "    num_threads=150,\n",
    "    num_candidate_programs=6,\n",
    "    max_labeled_demos=0,\n",
    "    max_bootstrapped_demos=3,\n",
    "    max_errors=10000,\n",
    ")\n",
    "\n",
    "sample_input = updated_devset[0]\n",
    "# print(sample_input.inputs())\n",
    "# print(encode_image(sample_input.inputs()[\"image_1\"]))\n",
    "mmmu = MMMUModule()\n",
    "print(sample_input.inputs())\n",
    "print(mmmu(**sample_input.inputs()))\n",
    "print(sample_input.answer)\n",
    "\n",
    "evaluate_mmmu = Evaluate(metric=answer_exact_match, num_threads=300, devset=updated_valset)\n",
    "lm.history[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "System message:\n",
      "\n",
      "Your input fields are:\n",
      "1. `question` (str): A question about the image(s)\n",
      "2. `image_1` (Image): An image relating to the shown problem\n",
      "3. `image_2` (Image): An image relating to the shown problem\n",
      "4. `answer_choices` (list[str]): The answer options for the question\n",
      "\n",
      "Your output fields are:\n",
      "1. `reasoning` (str)\n",
      "2. `answer` (str): The single letter of the correct answer. Do not include the entire answer or a period at the end.\n",
      "\n",
      "All interactions will be structured in the following way, with the appropriate values filled in.\n",
      "\n",
      "[[ ## question ## ]]\n",
      "{question}\n",
      "\n",
      "[[ ## image_1 ## ]]\n",
      "{image_1}\n",
      "\n",
      "[[ ## image_2 ## ]]\n",
      "{image_2}\n",
      "\n",
      "[[ ## answer_choices ## ]]\n",
      "{answer_choices}\n",
      "\n",
      "[[ ## reasoning ## ]]\n",
      "{reasoning}\n",
      "\n",
      "[[ ## answer ## ]]\n",
      "{answer}\n",
      "\n",
      "[[ ## completed ## ]]\n",
      "\n",
      "\n",
      "In adhering to this structure, your objective is: \n",
      "        Output a rationale and the answer to a multiple choice question about an image with the letter of the correct answer, if present, otherwise the exact answer.\n",
      "\n",
      "\n",
      "User message:\n",
      "\n",
      "[[ ## question ## ]]\n",
      "Church interiors from this time period typically were covered with <image 1> <image 2>\n",
      "[[ ## image_1 ## ]]\n",
      "<data:image/png;base64,<IMAGE BASE 64 ENCODED>>\n",
      "\n",
      "[[ ## image_2 ## ]]\n",
      "<data:image/png;base64,<IMAGE BASE 64 ENCODED>>\n",
      "\n",
      "[[ ## answer_choices ## ]]\n",
      "['A. timber roofs', 'B. quadripartite vaults', 'C. pendentive domes', 'D. masonry barrel vaults']\n",
      "Respond with the corresponding output fields, starting with the field `reasoning`, then `answer`, and then ending with the marker for `completed`.\n",
      "\n",
      "\n",
      "Response:\n",
      "\n",
      "[[ ## reasoning ## ]]\n",
      "The images depict church interiors that are characteristic of early Christian architecture. The second image shows a large open space with columns and a flat ceiling, which is typical of timber roofs. The first image, while showing the exterior, suggests a similar architectural style. Given the context of church interiors from this time period, the most appropriate answer is \"A. timber roofs,\" as they were commonly used in early Christian churches.\n",
      "\n",
      "[[ ## answer ## ]]\n",
      "A\n",
      "\n",
      "[[ ## completed ## ]]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lm.inspect_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make sure that multiple images work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PIL.PngImagePlugin.PngImageFile image mode=RGBA size=933x609 at 0x7E259C230790>\n",
      "<PIL.PngImagePlugin.PngImageFile image mode=RGBA size=933x737 at 0x7E259FF6C490>\n",
      "True\n",
      "False\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import PIL\n",
    "def set_image_to_black_square(example, key):\n",
    "    example_copy = example.copy()\n",
    "    example_copy[key] = PIL.Image.open(\"black_image_300x300.png\")\n",
    "    return example_copy.with_inputs(*example.inputs().keys())\n",
    "\n",
    "print(updated_devset[0][\"image_1\"])\n",
    "print(updated_devset[0][\"image_2\"])\n",
    "examples_no_image_1 = list(map(lambda x: set_image_to_black_square(x, \"image_1\"), updated_valset))\n",
    "print(examples_no_image_1[0][\"image_1\"] == PIL.Image.open(\"black_image_300x300.png\"))\n",
    "print(examples_no_image_1[0][\"image_2\"] == PIL.Image.open(\"black_image_300x300.png\"))\n",
    "examples_no_image_2 = list(map(lambda x: set_image_to_black_square(x, \"image_2\"), updated_valset))\n",
    "print(examples_no_image_2[0][\"image_1\"] == PIL.Image.open(\"black_image_300x300.png\"))\n",
    "print(examples_no_image_2[0][\"image_2\"] == PIL.Image.open(\"black_image_300x300.png\"))\n",
    "\n",
    "examples_no_actual_image = list(map(lambda x: set_image_to_black_square(x, \"image_1\"), updated_valset))\n",
    "examples_no_actual_image = list(map(lambda x: set_image_to_black_square(x, \"image_2\"), examples_no_actual_image))\n",
    "print(examples_no_actual_image[0][\"image_1\"] == PIL.Image.open(\"black_image_300x300.png\"))\n",
    "print(examples_no_actual_image[0][\"image_2\"] == PIL.Image.open(\"black_image_300x300.png\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example({'question': \"<image 1> What group of pathogens, often mistaken for regrowth following glyphosate treatment, can cause a growth habit in blackberry plants that is near-identical to the 'little leaf' symptoms commonly witnessed post-glyphosate treatment?\", 'options': '[\"I don\\'t know and I don\\'t want to guess\", \\'Nematodes\\', \\'Fungi\\', \\'Phytoplasmas\\', \\'Bacteria\\']', 'image_1': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=300x300 at 0x7E25782FD290>, 'image_2': <PIL.PngImagePlugin.PngImageFile image mode=P size=300x232 at 0x7E258579CF50>, 'answer_choices': '[\"A. I don\\'t know and I don\\'t want to guess\", \\'B. Nematodes\\', \\'C. Fungi\\', \\'D. Phytoplasmas\\', \\'E. Bacteria\\']'}) (input_keys={'image_1', 'answer_choices', 'image_2', 'options', 'question'})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction(\n",
      "    reasoning='The question asks about a group of pathogens that can cause symptoms in blackberry plants similar to those seen after glyphosate treatment. Among the options provided, phytoplasmas are known to cause growth abnormalities in plants, including symptoms that can be confused with glyphosate damage. Nematodes, fungi, and bacteria do not typically produce the same growth habit as described. Therefore, the most appropriate answer is D. Phytoplasmas.',\n",
      "    answer='D'\n",
      ")\n",
      "Example({'question': \"<image 1> What group of pathogens, often mistaken for regrowth following glyphosate treatment, can cause a growth habit in blackberry plants that is near-identical to the 'little leaf' symptoms commonly witnessed post-glyphosate treatment?\", 'options': '[\"I don\\'t know and I don\\'t want to guess\", \\'Nematodes\\', \\'Fungi\\', \\'Phytoplasmas\\', \\'Bacteria\\']', 'image_1': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=414x365 at 0x7E259E1FAA50>, 'image_2': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=300x300 at 0x7E259C2FA190>, 'answer_choices': '[\"A. I don\\'t know and I don\\'t want to guess\", \\'B. Nematodes\\', \\'C. Fungi\\', \\'D. Phytoplasmas\\', \\'E. Bacteria\\']'}) (input_keys={'image_1', 'answer_choices', 'image_2', 'options', 'question'})\n",
      "Prediction(\n",
      "    reasoning=\"The symptoms described in the question, which resemble 'little leaf' symptoms often seen after glyphosate treatment, are typically associated with phytoplasmas. Phytoplasmas are a group of pathogens that can cause similar growth abnormalities in plants, including blackberry plants. Other options like nematodes, fungi, and bacteria do not typically produce the same growth habit as phytoplasmas. Therefore, the correct answer is D. Phytoplasmas.\",\n",
      "    answer='D'\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "mmmu = MMMUModule()\n",
    "print(examples_no_image_1[0].inputs())\n",
    "print(mmmu(**examples_no_image_1[0].inputs()))\n",
    "\n",
    "print(examples_no_image_2[0].inputs())\n",
    "print(mmmu(**examples_no_image_2[0].inputs()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on MMMU validation set (N= 43 )\n",
      "Score with both images: 58.14\n",
      "Score with image_1 set to black square: 37.21\n",
      "Score with image_2 set to black square: 48.84\n",
      "Score with both images set to black squares: 44.19\n"
     ]
    }
   ],
   "source": [
    "normal = evaluate_mmmu(mmmu, devset=updated_valset)\n",
    "no_image_1 = evaluate_mmmu(mmmu, devset=examples_no_image_1)\n",
    "no_image_2 = evaluate_mmmu(mmmu, devset=examples_no_image_2)\n",
    "no_actual_image = evaluate_mmmu(mmmu, devset=examples_no_actual_image)\n",
    "print(\"Testing on MMMU validation set (N=\", len(updated_valset), \")\")\n",
    "print(\"Score with both images:\", normal)\n",
    "print(\"Score with image_1 set to black square:\", no_image_1)\n",
    "print(\"Score with image_2 set to black square:\", no_image_2)\n",
    "print(\"Score with both images set to black squares:\", no_actual_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Test with bootstrapped examples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make sure that JPGs work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert images to JPGs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original image format: PNG\n",
      "Converted image format: JPEG\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "from PIL import Image\n",
    "\n",
    "def convert_to_jpg(example):\n",
    "    example_copy = example.copy()\n",
    "    for key in ['image_1', 'image_2']:\n",
    "        if key in example_copy and isinstance(example_copy[key], Image.Image):\n",
    "            # Convert to RGB mode (in case it's not already)\n",
    "            img = example[key].convert('RGB')\n",
    "            \n",
    "            # Save as JPG in memory\n",
    "            buffer = io.BytesIO()\n",
    "            img.save(buffer, format='JPEG')\n",
    "            buffer.seek(0)\n",
    "            \n",
    "            # Load the JPG back as a PIL Image\n",
    "            example_copy[key] = Image.open(buffer)\n",
    "    \n",
    "    return example_copy.with_inputs(*example.inputs().keys())\n",
    "\n",
    "# Convert all images in the dataset to JPG\n",
    "examples_jpg = list(map(convert_to_jpg, updated_valset))\n",
    "\n",
    "# Verify conversion\n",
    "print(\"Original image format:\", updated_valset[0]['image_1'].format)\n",
    "print(\"Converted image format:\", examples_jpg[0]['image_1'].format)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example({'question': \"<image 1> What group of pathogens, often mistaken for regrowth following glyphosate treatment, can cause a growth habit in blackberry plants that is near-identical to the 'little leaf' symptoms commonly witnessed post-glyphosate treatment?\", 'options': '[\"I don\\'t know and I don\\'t want to guess\", \\'Nematodes\\', \\'Fungi\\', \\'Phytoplasmas\\', \\'Bacteria\\']', 'image_1': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=300x300 at 0x7E257B5A0650>, 'image_2': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=300x232 at 0x7E25921CAA10>, 'answer_choices': '[\"A. I don\\'t know and I don\\'t want to guess\", \\'B. Nematodes\\', \\'C. Fungi\\', \\'D. Phytoplasmas\\', \\'E. Bacteria\\']'}) (input_keys={'image_1', 'answer_choices', 'image_2', 'options', 'question'})\n",
      "Prediction(\n",
      "    reasoning=\"The question asks about a group of pathogens that can cause symptoms in blackberry plants similar to those seen after glyphosate treatment. Among the options provided, phytoplasmas are known to cause growth abnormalities in plants, including symptoms resembling 'little leaf' conditions. Nematodes, fungi, and bacteria do not typically cause such specific symptoms in this context. Therefore, the most appropriate answer is D. Phytoplasmas.\",\n",
      "    answer='D'\n",
      ")\n",
      "JPEG\n"
     ]
    }
   ],
   "source": [
    "examples_jpg = list(map(convert_to_jpg, updated_valset))\n",
    "examples_no_image_1_jpg = list(map(lambda x: convert_to_jpg(x), examples_no_image_1))\n",
    "examples_no_image_2_jpg = list(map(lambda x: convert_to_jpg(x), examples_no_image_2))\n",
    "examples_no_actual_image_jpg = list(map(lambda x: convert_to_jpg(x), examples_no_actual_image))\n",
    "\n",
    "mmmu = MMMUModule()\n",
    "print(examples_no_image_1_jpg[0].inputs())\n",
    "print(mmmu(**examples_no_image_1_jpg[0].inputs()))\n",
    "print(examples_no_image_1_jpg[0][\"image_1\"].format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on MMMU validation set (N= 43 )\n",
      "Score with both images: 58.14\n",
      "Score with image_1 set to black square: 44.19\n",
      "Score with image_2 set to black square: 46.51\n",
      "Score with both images set to black squares: 44.19\n"
     ]
    }
   ],
   "source": [
    "normal = evaluate_mmmu(mmmu, devset=examples_jpg)\n",
    "no_image_1 = evaluate_mmmu(mmmu, devset=examples_no_image_1_jpg)\n",
    "no_image_2 = evaluate_mmmu(mmmu, devset=examples_no_image_2_jpg)\n",
    "no_actual_image = evaluate_mmmu(mmmu, devset=examples_no_actual_image_jpg)\n",
    "print(\"Testing on MMMU validation set (N=\", len(updated_valset), \")\")\n",
    "print(\"Score with both images:\", normal)\n",
    "print(\"Score with image_1 set to black square:\", no_image_1)\n",
    "print(\"Score with image_2 set to black square:\", no_image_2)\n",
    "print(\"Score with both images set to black squares:\", no_actual_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "System message:\n",
      "\n",
      "Your input fields are:\n",
      "1. `question` (str): A question about the image(s)\n",
      "2. `image_1` (Image): An image relating to the shown problem\n",
      "3. `image_2` (Image): An image relating to the shown problem\n",
      "4. `answer_choices` (list[str]): The answer options for the question\n",
      "\n",
      "Your output fields are:\n",
      "1. `reasoning` (str)\n",
      "2. `answer` (str): The single letter of the correct answer. Do not include the entire answer or a period at the end.\n",
      "\n",
      "All interactions will be structured in the following way, with the appropriate values filled in.\n",
      "\n",
      "[[ ## question ## ]]\n",
      "{question}\n",
      "\n",
      "[[ ## image_1 ## ]]\n",
      "{image_1}\n",
      "\n",
      "[[ ## image_2 ## ]]\n",
      "{image_2}\n",
      "\n",
      "[[ ## answer_choices ## ]]\n",
      "{answer_choices}\n",
      "\n",
      "[[ ## reasoning ## ]]\n",
      "{reasoning}\n",
      "\n",
      "[[ ## answer ## ]]\n",
      "{answer}\n",
      "\n",
      "[[ ## completed ## ]]\n",
      "\n",
      "\n",
      "In adhering to this structure, your objective is: \n",
      "        Output a rationale and the answer to a multiple choice question about an image with the letter of the correct answer, if present, otherwise the exact answer.\n",
      "\n",
      "\n",
      "User message:\n",
      "\n",
      "[[ ## question ## ]]\n",
      "<image 1> Standard reduction potentials for the half-reactions associated with the electrochemical cell shown above are given in the table below. <image 2> Which of the following is the net ionic equation for the overall reaction that occurs as the cell operates?\n",
      "[[ ## image_1 ## ]]\n",
      "<data:image/jpeg;base64,<IMAGE BASE 64 ENCODED>>\n",
      "\n",
      "[[ ## image_2 ## ]]\n",
      "<data:image/jpeg;base64,<IMAGE BASE 64 ENCODED>>\n",
      "\n",
      "[[ ## answer_choices ## ]]\n",
      "['A. $Ag$(s) + $Zn^{2+}$(aq) $\\\\rightarrow$ $Ag^{2+}$(aq) + $Zn$(s)', 'B. $Ag^+$(aq) + $Zn^{2+}$(aq) + 3 $e^-$ $\\\\rightarrow$ $AgZn$(s)', 'C. $Ag^+$(aq) + $Zn$(s) $\\\\rightarrow$ $Ag$(s) + $Zn^{2+}$(aq) + $e^-$', 'D. 2 $Ag^+$(aq) + $Zn$(s) $\\\\\\rightarrow$ 2 $Ag$(s) + $Zn^{2+}$(aq)']\n",
      "Respond with the corresponding output fields, starting with the field `reasoning`, then `answer`, and then ending with the marker for `completed`.\n",
      "\n",
      "\n",
      "Response:\n",
      "\n",
      "[[ ## reasoning ## ]]\n",
      "To determine the net ionic equation for the overall reaction in the electrochemical cell, we need to consider the half-reactions involved. The reduction half-reaction typically involves the gain of electrons, while the oxidation half-reaction involves the loss of electrons. \n",
      "\n",
      "From the standard reduction potentials, we can identify that silver ions ($Ag^+$) are reduced to solid silver ($Ag$), and zinc metal ($Zn$) is oxidized to zinc ions ($Zn^{2+}$). The balanced overall reaction will involve two moles of silver ions reacting with one mole of zinc metal, resulting in two moles of solid silver and one mole of zinc ions.\n",
      "\n",
      "The correct net ionic equation that represents this process is:\n",
      "\\[ 2 Ag^+(aq) + Zn(s) \\rightarrow 2 Ag(s) + Zn^{2+}(aq) \\]\n",
      "\n",
      "This matches option D.\n",
      "\n",
      "[[ ## answer ## ]]\n",
      "D\n",
      "\n",
      "[[ ## completed ## ]]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lm.inspect_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing that URLs work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "colors = {\n",
    "    \"White\": \"FFFFFF\",\n",
    "    \"Red\": \"FF0000\",\n",
    "    \"Green\": \"00FF00\",\n",
    "    \"Blue\": \"0000FF\",\n",
    "    \"Yellow\": \"FFFF00\",\n",
    "    \"Cyan\": \"00FFFF\",\n",
    "    \"Magenta\": \"FF00FF\",\n",
    "    \"Gray\": \"808080\",\n",
    "    \"Orange\": \"FFA500\",\n",
    "    \"Purple\": \"800080\"\n",
    "}\n",
    "def get_color_image_url(color, file_extension=\"png\"):\n",
    "    return f\"https://placehold.co/300/{colors[color]}/{colors[color]}.{file_extension}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example({'image_1': 'https://placehold.co/300/FFFF00/FFFF00.png', 'image_2': 'https://placehold.co/300/0000FF/0000FF.png', 'question': 'What color is image_2?', 'answer': 'Blue'}) (input_keys={'image_1', 'image_2', 'question'})\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def generate_random_2_color_image_examples(n):\n",
    "    examples = []\n",
    "    for _ in range(n):\n",
    "        color_1, color_2 = random.sample(list(colors.keys()), 2)\n",
    "        chosen_color = color_1 if random.random() < 0.5 else color_2\n",
    "        chosen_image = \"image_1\" if chosen_color == color_1 else \"image_2\"\n",
    "        example_kwargs = {\n",
    "            \"image_1\": get_color_image_url(color_1),\n",
    "            \"image_2\": get_color_image_url(color_2),\n",
    "            \"question\": f\"What color is {chosen_image}?\",\n",
    "            \"answer\": chosen_color\n",
    "        }\n",
    "        examples.append(dspy.Example(**example_kwargs).with_inputs(\"image_1\", \"image_2\", \"question\"))\n",
    "    return examples\n",
    "\n",
    "examples = generate_random_2_color_image_examples(100)\n",
    "print(examples[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColorSignature(dspy.Signature):\n",
    "    \"\"\"Output the color of the designated image.\"\"\"\n",
    "    image_1: dspy.Image = dspy.InputField(desc=\"An image\")\n",
    "    image_2: dspy.Image = dspy.InputField(desc=\"An image\")\n",
    "    question: str = dspy.InputField(desc=\"A question about the image\")\n",
    "    answer: str = dspy.OutputField(desc=\"The color of the designated image\")\n",
    "color_program = dspy.Predict(ColorSignature)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example({'image_1': 'https://placehold.co/300/FFFF00/FFFF00.png', 'image_2': 'https://placehold.co/300/0000FF/0000FF.png', 'question': 'What color is image_2?', 'answer': 'Blue'}) (input_keys={'image_1', 'image_2', 'question'})\n",
      "Prediction(\n",
      "    reasoning='The color of image_2 is a solid blue shade.',\n",
      "    answer='Blue'\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(examples[0])\n",
    "print(color_program(**examples[0].inputs()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_optimizer = dspy.BootstrapFewShot(metric=answer_exact_match, max_bootstrapped_demos=3, max_labeled_demos=10)\n",
    "smaller_few_shot_optimizer = dspy.BootstrapFewShot(metric=answer_exact_match, max_bootstrapped_demos=1, max_labeled_demos=1)\n",
    "dataset = generate_random_2_color_image_examples(1000)\n",
    "trainset = dataset[:200]\n",
    "validationset = dataset[200:400]\n",
    "evaluate_colors = Evaluate(metric=answer_exact_match, num_threads=300, devset=validationset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 3/200 [00:18<20:27,  6.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 3 full traces after 4 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/200 [00:02<08:09,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 2 examples in round 0.\n",
      "99.0\n",
      "100.0\n",
      "96.5\n"
     ]
    }
   ],
   "source": [
    "compiled_color_program = few_shot_optimizer.compile(color_program, trainset=trainset)\n",
    "compiled_smaller_color_program = smaller_few_shot_optimizer.compile(color_program, trainset=trainset)\n",
    "print(evaluate_colors(color_program))\n",
    "print(evaluate_colors(compiled_color_program))\n",
    "print(evaluate_colors(compiled_smaller_color_program))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction(\n",
      "    reasoning='Not supplied for this particular example.',\n",
      "    answer='White'\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "System message:\n",
      "\n",
      "Your input fields are:\n",
      "1. `image_1` (Image): An image\n",
      "2. `image_2` (Image): An image\n",
      "3. `question` (str): A question about the image\n",
      "\n",
      "Your output fields are:\n",
      "1. `reasoning` (str)\n",
      "2. `answer` (str): The color of the designated image\n",
      "\n",
      "All interactions will be structured in the following way, with the appropriate values filled in.\n",
      "\n",
      "[[ ## image_1 ## ]]\n",
      "{image_1}\n",
      "\n",
      "[[ ## image_2 ## ]]\n",
      "{image_2}\n",
      "\n",
      "[[ ## question ## ]]\n",
      "{question}\n",
      "\n",
      "[[ ## reasoning ## ]]\n",
      "{reasoning}\n",
      "\n",
      "[[ ## answer ## ]]\n",
      "{answer}\n",
      "\n",
      "[[ ## completed ## ]]\n",
      "\n",
      "\n",
      "In adhering to this structure, your objective is: \n",
      "        Output the color of the designated image.\n",
      "\n",
      "\n",
      "User message:\n",
      "\n",
      "This is an example of the task, though some input or output fields are not supplied.\n",
      "[[ ## image_1 ## ]]\n",
      "<image_url: https://placehold.co/300/00FF00/00FF00.png>\n",
      "\n",
      "[[ ## image_2 ## ]]\n",
      "<image_url: https://placehold.co/300/800080/800080.png>\n",
      "\n",
      "[[ ## question ## ]]\n",
      "What color is image_2?\n",
      "Respond with the corresponding output fields, starting with the field `reasoning`, then `answer`, and then ending with the marker for `completed`.\n",
      "\n",
      "\n",
      "Assistant message:\n",
      "\n",
      "[[ ## reasoning ## ]]\n",
      "Not supplied for this particular example.\n",
      "[[ ## answer ## ]]\n",
      "Purple\n",
      "[[ ## completed ## ]]\n",
      "\n",
      "\n",
      "User message:\n",
      "\n",
      "This is an example of the task, though some input or output fields are not supplied.\n",
      "[[ ## image_1 ## ]]\n",
      "<image_url: https://placehold.co/300/FF0000/FF0000.png>\n",
      "\n",
      "[[ ## image_2 ## ]]\n",
      "<image_url: https://placehold.co/300/FFA500/FFA500.png>\n",
      "\n",
      "[[ ## question ## ]]\n",
      "What color is image_2?\n",
      "Respond with the corresponding output fields, starting with the field `reasoning`, then `answer`, and then ending with the marker for `completed`.\n",
      "\n",
      "\n",
      "Assistant message:\n",
      "\n",
      "[[ ## reasoning ## ]]\n",
      "Not supplied for this particular example.\n",
      "[[ ## answer ## ]]\n",
      "Orange\n",
      "[[ ## completed ## ]]\n",
      "\n",
      "\n",
      "User message:\n",
      "\n",
      "This is an example of the task, though some input or output fields are not supplied.\n",
      "[[ ## image_1 ## ]]\n",
      "<image_url: https://placehold.co/300/FFA500/FFA500.png>\n",
      "\n",
      "[[ ## image_2 ## ]]\n",
      "<image_url: https://placehold.co/300/00FFFF/00FFFF.png>\n",
      "\n",
      "[[ ## question ## ]]\n",
      "What color is image_2?\n",
      "Respond with the corresponding output fields, starting with the field `reasoning`, then `answer`, and then ending with the marker for `completed`.\n",
      "\n",
      "\n",
      "Assistant message:\n",
      "\n",
      "[[ ## reasoning ## ]]\n",
      "Not supplied for this particular example.\n",
      "[[ ## answer ## ]]\n",
      "Cyan\n",
      "[[ ## completed ## ]]\n",
      "\n",
      "\n",
      "User message:\n",
      "\n",
      "This is an example of the task, though some input or output fields are not supplied.\n",
      "[[ ## image_1 ## ]]\n",
      "<image_url: https://placehold.co/300/FF00FF/FF00FF.png>\n",
      "\n",
      "[[ ## image_2 ## ]]\n",
      "<image_url: https://placehold.co/300/0000FF/0000FF.png>\n",
      "\n",
      "[[ ## question ## ]]\n",
      "What color is image_2?\n",
      "Respond with the corresponding output fields, starting with the field `reasoning`, then `answer`, and then ending with the marker for `completed`.\n",
      "\n",
      "\n",
      "Assistant message:\n",
      "\n",
      "[[ ## reasoning ## ]]\n",
      "Not supplied for this particular example.\n",
      "[[ ## answer ## ]]\n",
      "Blue\n",
      "[[ ## completed ## ]]\n",
      "\n",
      "\n",
      "User message:\n",
      "\n",
      "This is an example of the task, though some input or output fields are not supplied.\n",
      "[[ ## image_1 ## ]]\n",
      "<image_url: https://placehold.co/300/FF0000/FF0000.png>\n",
      "\n",
      "[[ ## image_2 ## ]]\n",
      "<image_url: https://placehold.co/300/FFA500/FFA500.png>\n",
      "\n",
      "[[ ## question ## ]]\n",
      "What color is image_1?\n",
      "Respond with the corresponding output fields, starting with the field `reasoning`, then `answer`, and then ending with the marker for `completed`.\n",
      "\n",
      "\n",
      "Assistant message:\n",
      "\n",
      "[[ ## reasoning ## ]]\n",
      "Not supplied for this particular example.\n",
      "[[ ## answer ## ]]\n",
      "Red\n",
      "[[ ## completed ## ]]\n",
      "\n",
      "\n",
      "User message:\n",
      "\n",
      "This is an example of the task, though some input or output fields are not supplied.\n",
      "[[ ## image_1 ## ]]\n",
      "<image_url: https://placehold.co/300/FF0000/FF0000.png>\n",
      "\n",
      "[[ ## image_2 ## ]]\n",
      "<image_url: https://placehold.co/300/FFFF00/FFFF00.png>\n",
      "\n",
      "[[ ## question ## ]]\n",
      "What color is image_2?\n",
      "Respond with the corresponding output fields, starting with the field `reasoning`, then `answer`, and then ending with the marker for `completed`.\n",
      "\n",
      "\n",
      "Assistant message:\n",
      "\n",
      "[[ ## reasoning ## ]]\n",
      "Not supplied for this particular example.\n",
      "[[ ## answer ## ]]\n",
      "Yellow\n",
      "[[ ## completed ## ]]\n",
      "\n",
      "\n",
      "User message:\n",
      "\n",
      "This is an example of the task, though some input or output fields are not supplied.\n",
      "[[ ## image_1 ## ]]\n",
      "<image_url: https://placehold.co/300/FFFFFF/FFFFFF.png>\n",
      "\n",
      "[[ ## image_2 ## ]]\n",
      "<image_url: https://placehold.co/300/800080/800080.png>\n",
      "\n",
      "[[ ## question ## ]]\n",
      "What color is image_1?\n",
      "Respond with the corresponding output fields, starting with the field `reasoning`, then `answer`, and then ending with the marker for `completed`.\n",
      "\n",
      "\n",
      "Assistant message:\n",
      "\n",
      "[[ ## reasoning ## ]]\n",
      "Not supplied for this particular example.\n",
      "[[ ## answer ## ]]\n",
      "White\n",
      "[[ ## completed ## ]]\n",
      "\n",
      "\n",
      "User message:\n",
      "\n",
      "[[ ## image_1 ## ]]\n",
      "<image_url: https://placehold.co/300/800080/800080.png>\n",
      "\n",
      "[[ ## image_2 ## ]]\n",
      "<image_url: https://placehold.co/300/FF0000/FF0000.png>\n",
      "\n",
      "[[ ## question ## ]]\n",
      "What color is image_2?\n",
      "Respond with the corresponding output fields, starting with the field `reasoning`, then `answer`, and then ending with the marker for `completed`.\n",
      "\n",
      "\n",
      "Assistant message:\n",
      "\n",
      "[[ ## reasoning ## ]]\n",
      "Not supplied for this particular example.\n",
      "[[ ## answer ## ]]\n",
      "Red\n",
      "[[ ## completed ## ]]\n",
      "\n",
      "\n",
      "User message:\n",
      "\n",
      "[[ ## image_1 ## ]]\n",
      "<image_url: https://placehold.co/300/FFFFFF/FFFFFF.png>\n",
      "\n",
      "[[ ## image_2 ## ]]\n",
      "<image_url: https://placehold.co/300/00FF00/00FF00.png>\n",
      "\n",
      "[[ ## question ## ]]\n",
      "What color is image_1?\n",
      "Respond with the corresponding output fields, starting with the field `reasoning`, then `answer`, and then ending with the marker for `completed`.\n",
      "\n",
      "\n",
      "Assistant message:\n",
      "\n",
      "[[ ## reasoning ## ]]\n",
      "Not supplied for this particular example.\n",
      "[[ ## answer ## ]]\n",
      "White\n",
      "[[ ## completed ## ]]\n",
      "\n",
      "\n",
      "User message:\n",
      "\n",
      "[[ ## image_1 ## ]]\n",
      "<image_url: https://placehold.co/300/FFA500/FFA500.png>\n",
      "\n",
      "[[ ## image_2 ## ]]\n",
      "<image_url: https://placehold.co/300/00FFFF/00FFFF.png>\n",
      "\n",
      "[[ ## question ## ]]\n",
      "What color is image_1?\n",
      "Respond with the corresponding output fields, starting with the field `reasoning`, then `answer`, and then ending with the marker for `completed`.\n",
      "\n",
      "\n",
      "Assistant message:\n",
      "\n",
      "[[ ## reasoning ## ]]\n",
      "Not supplied for this particular example.\n",
      "[[ ## answer ## ]]\n",
      "Orange\n",
      "[[ ## completed ## ]]\n",
      "\n",
      "\n",
      "User message:\n",
      "\n",
      "[[ ## image_1 ## ]]\n",
      "<image_url: https://placehold.co/300/FFFFFF/FFFFFF.png>\n",
      "\n",
      "[[ ## image_2 ## ]]\n",
      "<image_url: https://placehold.co/300/800080/800080.png>\n",
      "\n",
      "[[ ## question ## ]]\n",
      "What color is image_1?\n",
      "Respond with the corresponding output fields, starting with the field `reasoning`, then `answer`, and then ending with the marker for `completed`.\n",
      "\n",
      "\n",
      "Response:\n",
      "\n",
      "[[ ## reasoning ## ]]\n",
      "Not supplied for this particular example.\n",
      "[[ ## answer ## ]]\n",
      "White\n",
      "[[ ## completed ## ]]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(compiled_color_program(**validationset[0].inputs()))\n",
    "lm.inspect_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO(Isaac): Delete; Archive of old experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DataLoader().from_huggingface(\"Alanox/stanford-dogs\", split=\"full\", input_keys=(\"image\",), trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename the field from \"image\" to \"image_1\"\n",
    "def rename_field(example, old_name, new_name):\n",
    "    try:\n",
    "        example[new_name] = example[old_name]\n",
    "        del example[old_name]\n",
    "    except Exception:\n",
    "        pass\n",
    "    return example\n",
    "    \n",
    "dog_dataset = list(map(rename_field, dataset, [\"image\"]*len(dataset), [\"image_1\"]*len(dataset)))\n",
    "dog_dataset2 = list(map(rename_field, dog_dataset, [\"target\"]*len(dog_dataset), [\"answer\"]*len(dog_dataset)))\n",
    "dog_dataset3 = list(map(lambda x: x.with_inputs(\"image_1\"), dog_dataset2))\n",
    "dog_dataset = dog_dataset3\n",
    "random.shuffle(dog_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction(\n",
      "    reasoning='The dog in the image has a curly, white coat and a distinctive blue collar, which are characteristic features of the Bedlington Terrier breed.',\n",
      "    answer='Bedlington Terrier'\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class DogPictureSignature(dspy.Signature):\n",
    "    \"\"\"Output the dog breed of the dog in the image.\"\"\"\n",
    "    image_1: dspy.Image = dspy.InputField(desc=\"An image of a dog\")\n",
    "    answer: str = dspy.OutputField(desc=\"The dog breed of the dog in the image\")\n",
    "\n",
    "class DogPicture(dspy.Module):\n",
    "    def __init__(self) -> None:\n",
    "        self.predictor = dspy.ChainOfThought(DogPictureSignature)\n",
    "    \n",
    "    def __call__(self, **kwargs):\n",
    "        return self.predictor(**kwargs)\n",
    "\n",
    "dog_picture = DogPicture()\n",
    "print(dog_picture(**dog_dataset[0].inputs()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate = Evaluate(metric=answer_exact_match, num_threads=100, devset= dog_dataset[-500:], display_progress=True, max_errors=10000)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
